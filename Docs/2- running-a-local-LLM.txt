1. Installing Ollama:
    * Download and install the "jetson-containers" utilities:
        > git clone https://github.com/dusty-nv/jetson-containers
        > bash jetson-containers/install.sh

    * Run the command: jetson-containers run --name ollama $(autotag ollama)

2. Running an LLM Locally on CLI:
    * Run the command: ollama run llama3.2:3b
    * Run /bye to end this session

3. Running an LLM Locally on Open WebUI:
    * Ollama is already installed
    * Run this command:
        > sudo docker run -d --network=host \
            -v ${HOME}/open-webui:/app/backend/data \
            -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \
            --name open-webui \
            --restart always \
            ghcr.io/open-webui/open-webui:main

    * Go to: http://JETSON_IP:8080
